{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNXAdPt/99LekJCPdbkYJyg"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Introduction: \"Large Language Models\"  "
      ],
      "metadata": {
        "id": "6VkCnBURdJM5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **1. Evolution to Large Language Models (LLMs)**  \n",
        "- **From Task-Specific to General-Purpose Models**:  \n",
        "  - Early NLP systems relied on **task-specific architectures** (e.g., n-grams, RNNs).  \n",
        "  - Transformers (Chapter 9) enabled **scalable, general-purpose models** trained on massive text corpora, leading to the rise of LLMs like GPT, BERT, and T5.  \n",
        "\n",
        "- **Scale as a Catalyst**:  \n",
        "  - **Data**: LLMs are trained on **trillions of tokens** from diverse sources (books, web text, code).  \n",
        "  - **Parameters**: Model sizes range from hundreds of millions (BERT) to hundreds of billions (GPT-4) of parameters.  \n",
        "  - **Compute**: Training leverages **GPU/TPU clusters**, enabling unprecedented parallelism.  \n",
        "\n",
        "---\n",
        "\n",
        "#### **2. Core Concepts in LLM Pretraining**  \n",
        "1. **Self-Supervised Learning**:  \n",
        "   - LLMs learn by predicting masked tokens (BERT) or next tokens (GPT) in unlabeled text, eliminating the need for manual annotation.  \n",
        "2. **Emergent Abilities**:  \n",
        "   - At scale, LLMs exhibit **few-shot learning**, **reasoning**, and **in-context adaptation** without explicit fine-tuning.  \n",
        "3. **Architectural Foundations**:  \n",
        "   - **Autoregressive Models** (e.g., GPT): Generate text left-to-right using causal attention masks.  \n",
        "   - **Bidirectional Models** (e.g., BERT): Masked language modeling captures context from both directions.  \n",
        "\n",
        "---\n",
        "\n",
        "#### **3. Impact and Applications**  \n",
        "- **Versatility**: LLMs power diverse applications:  \n",
        "  - **Text Generation** (dialogue, stories, code).  \n",
        "  - **Information Retrieval** (question answering, summarization).  \n",
        "  - **Multimodal Tasks** (text-to-image, text-to-speech).  \n",
        "- **Democratization**: Open-source models (e.g., LLaMA, Mistral) and APIs (e.g., ChatGPT) make LLMs accessible to developers and researchers.  \n",
        "\n",
        "---\n",
        "\n",
        "#### **4. Challenges and Considerations**  \n",
        "- **Computational Costs**: Training LLMs requires massive infrastructure (e.g., months on thousands of GPUs).  \n",
        "- **Ethical Risks**:  \n",
        "  - **Bias**: Models may perpetuate harmful stereotypes from training data.  \n",
        "  - **Misinformation**: Potential for generating plausible but false content.  \n",
        "  - **Environmental Impact**: High energy consumption during training/inference.  \n"
      ],
      "metadata": {
        "id": "u_zZoBXKdEEl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 10.1 Foundations of Large Language Models (LLMs)"
      ],
      "metadata": {
        "id": "lbhAqKbnk8mG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **1. Architectural Scaling**  \n",
        "- **Transformer Enhancements**:  \n",
        "  - LLMs scale the core Transformer architecture (Chapter 9) by increasing **depth** (stacking more layers, e.g., 96+), **width** (larger hidden dimensions), and **attention heads** (e.g., 64–128 heads).  \n",
        "  - Example: GPT-3 uses 96 Transformer layers with 12,288-dimensional hidden states.  \n",
        "- **Efficiency Optimizations**:  \n",
        "  - **Sparse Attention**: Reduces computational complexity for long sequences (e.g., restricting attention to local windows).  \n",
        "  - **Mixed Precision Training**: Uses 16-bit or 8-bit floating-point numbers to speed up computation and reduce memory usage.  \n",
        "\n",
        "---\n",
        "\n",
        "#### **2. Data Scaling and Pretraining**  \n",
        "- **Massive Datasets**:  \n",
        "  - LLMs train on **trillions of tokens** from diverse sources (books, web text, code, scientific papers).  \n",
        "  - Datasets are carefully filtered to remove low-quality or harmful content.  \n",
        "- **Tokenization**:  \n",
        "  - Subword tokenization (e.g., BPE, SentencePiece) balances vocabulary size and out-of-vocabulary robustness.  \n",
        "  - Vocabulary sizes range from 50k to 500k tokens, depending on the model.  \n",
        "\n",
        "---\n",
        "\n",
        "#### **3. Scaling Laws**  \n",
        "- **Compute-Optimal Training**:  \n",
        "  - **Chinchilla Laws** (Hoffmann et al., 2022): For a fixed compute budget, model size and training data should scale proportionally. For example, doubling model parameters requires doubling training tokens.  \n",
        "  - **Emergent Abilities**: Larger models exhibit unforeseen capabilities (e.g., arithmetic, code generation) not present in smaller variants.  \n",
        "- **Performance Trends**:  \n",
        "  - Model performance improves predictably with increases in **model size**, **data volume**, and **training compute** (power-law relationships).  \n",
        "\n",
        "---\n",
        "\n",
        "#### **4. Training Infrastructure**  \n",
        "- **Hardware**:  \n",
        "  - LLMs require **GPU/TPU clusters** (thousands of devices) for distributed training over weeks or months.  \n",
        "  - Techniques like **model parallelism** (splitting layers across devices) and **pipeline parallelism** (splitting batches) enable scalability.  \n",
        "- **Optimization**:  \n",
        "  - **AdamW Optimizer**: Adapts learning rates dynamically with weight decay for regularization.  \n",
        "  - **Gradient Checkpointing**: Reduces memory usage by recomputing intermediate activations during backpropagation.  \n",
        "\n",
        "---\n",
        "\n",
        "#### **5. Challenges and Considerations**  \n",
        "- **Environmental Impact**:  \n",
        "  - Training LLMs consumes massive energy (e.g., GPT-3’s carbon footprint ≈ 500 tons of CO₂).  \n",
        "- **Bias and Fairness**:  \n",
        "  - Training data often reflects societal biases, requiring post-hoc mitigation (e.g., debiasing filters, RLHF).  \n",
        "- **Cost Barriers**:  \n",
        "  - High computational and financial costs centralize LLM development in resource-rich organizations.  \n",
        "\n",
        "---\n",
        "\n",
        "### **Connection to Broader Context**  \n",
        "- **Builds on Chapter 9**: LLMs extend the Transformer’s architecture through scaling and optimization.  \n",
        "- **Sets Stage for Pretraining**: Section 10.1 underpins later sections on fine-tuning (Chapter 11) and alignment (Chapter 12).  \n",
        "- **Real-World Impact**: LLMs drive advancements in chatbots (Chapter 15), translation (Chapter 13), and speech systems (Chapter 16).  "
      ],
      "metadata": {
        "id": "rh2Zndzhk559"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 10.2 Sampling for LLM Generation"
      ],
      "metadata": {
        "id": "UoiKjrFWAo2k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. **Decoding as Autoregressive Generation**\n",
        "\n",
        "   * **Decoding** is the process of choosing the next word to generate based on the model’s predicted probability distribution over the vocabulary.\n",
        "   * When done in a left-to-right (or right-to-left) fashion, conditioning each choice on all previous choices, it’s called **autoregressive** or **causal** generation .\n",
        "\n",
        "2. **Random Sampling and Its Limitations**\n",
        "\n",
        "   * **Random sampling** selects each next token by drawing from the full probability distribution. Although this favors high-probability words, the long “tail” of low-probability words can lead to bizarre or incoherent outputs.&#x20;\n",
        "   * **Greedy decoding**, the special case of always picking the single most likely word (the arg max), is deterministic and often yields overly generic, repetitive text .\n",
        "\n",
        "3. **Balancing Quality and Diversity**\n",
        "\n",
        "   * All sampling methods introduce hyperparameters that trade off **quality** (coherence, factuality) against **diversity** (creativity, variety). Emphasizing only the highest-probability words yields safe but dull text; allowing more mid-probability words increases variety but risks incoherence .\n",
        "\n",
        "4. **Top-k Sampling**\n",
        "\n",
        "   * **Top-k sampling** truncates the distribution to the *k* most probable tokens, renormalizes their probabilities, and samples from this reduced set.\n",
        "   * When k=1, it reduces to greedy decoding; larger k increases diversity while still avoiding extremely unlikely words .\n",
        "\n",
        "5. **Nucleus (Top-p) Sampling**\n",
        "\n",
        "   * **Top-p (nucleus) sampling** dynamically selects the smallest set of tokens whose cumulative probability mass ≥ p, then samples from this set.\n",
        "   * This adapts to context by sometimes including more candidates when the distribution is flat and fewer when it’s peaky .\n",
        "\n",
        "6. **Temperature Sampling**\n",
        "\n",
        "   * **Temperature** τ controls distribution “sharpness” by dividing the model’s logits by τ before the softmax:\n",
        "     y=softmax(u/τ)\n",
        "   * **Low temperatures** (τ<1) make the distribution peakier (more greedy), boosting top-probability tokens; **high temperatures** (τ>1) flatten the distribution, encouraging exploration of lower-probability words .\n",
        "\n",
        "---\n",
        "\n",
        "These three methods—top-k, top-p, and temperature sampling—are the primary levers practitioners use to shape LLM outputs for different applications, from safe generation to creative writing.\n"
      ],
      "metadata": {
        "id": "NcMkf2LiApb9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 10.3 Pretraining Large Language Models"
      ],
      "metadata": {
        "id": "Lz51CdjVBh37"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "1. **Self-Supervised Training (10.3.1)**\n",
        "\n",
        "   * Large language models are trained to predict the next token in text using **teacher forcing**, where at each position the model is given the true history and learns to assign high probability to the actual next word.\n",
        "   * Training minimizes the **cross-entropy loss** between the model’s predicted distribution and the one-hot true next token:\n",
        "     ![image.png](data:image/png;base64,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).\n",
        "   * This **self-supervision** requires no external labels—text itself provides the training signal.\n",
        "\n",
        "2. **Pretraining Data and Filtering (10.3.2)**\n",
        "\n",
        "   * Pretraining draws on **hundreds of billions of tokens** scraped from the web (e.g., Common Crawl, The Pile) and other corpora like books and Wikipedia.\n",
        "   * **Quality filters** (often classifier-based) score and remove low-quality or boilerplate text; **deduplication** eliminates repeated content.\n",
        "   * **Safety filters** (e.g., toxicity detectors) aim to reduce harmful or personally identifiable information, though current classifiers can introduce biases and aren’t foolproof .\n",
        "   * Ethical and legal considerations include **copyright** (fair use ambiguity), **data consent** (robots.txt and ToS “no-crawl” flags), and **privacy** (residual PII in web data) .\n",
        "\n",
        "3. **Finetuning after Pretraining (10.3.3)**\n",
        "\n",
        "   * To specialize models for new domains or tasks (e.g., medical text, legal language), one can **continue training** (“continued pretraining”) on in-domain data, updating **all** model parameters.\n",
        "   * **Parameter-efficient finetuning** freezes most weights and updates only a small subset (e.g., adapters, LoRA layers), drastically reducing compute and storage costs.\n",
        "   * For **task-specific objectives** (classification, sequence labeling), models are often equipped with new heads and trained on supervised data, with either full or partial parameter updates.\n",
        "   * Collectively, these steps—continued pretraining, PEFT, and supervised finetuning—are referred to as **post-training**, and enable large language models to adapt to specialized use cases .\n"
      ],
      "metadata": {
        "id": "eUclhsLVBf7B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 10.4 Evaluating Large Language Models"
      ],
      "metadata": {
        "id": "DPgh26fACLau"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "1. **Perplexity**\n",
        "\n",
        "   * Perplexity measures how well a model predicts unseen text: for a test sequence $w_{1:n}$,\n",
        "\n",
        "    ![image.png](data:image/png;base64,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)\n",
        "   * Lower perplexity indicates better predictive power. Since it depends on tokenization, it’s only fair to compare models using the same tokenizer .\n",
        "\n",
        "2. **Downstream Task Accuracy**\n",
        "\n",
        "   * Beyond intrinsic metrics like perplexity, LLMs are evaluated on concrete tasks—machine translation, summarization, question answering, speech recognition, dialogue, etc.—using task-specific metrics (e.g., BLEU, ROUGE, exact match). These evaluations gauge how improvements in the model translate into real-world performance .\n",
        "\n",
        "3. **Efficiency & Resource Usage**\n",
        "\n",
        "   * Model size, training and inference speed, and memory requirements are critical constraints. Evaluation often normalizes performance to a compute or memory budget.\n",
        "   * Energy consumption can be directly measured (e.g., kWh or CO₂ emissions) to assess environmental and cost impacts .\n",
        "\n",
        "4. **Fairness, Bias & Robustness**\n",
        "\n",
        "   * LLMs exhibit demographic and societal biases. Benchmarks such as StereoSet, RealToxicityPrompts, and BBQ measure stereotypical or toxic outputs.\n",
        "   * Fairness can be formalized (e.g., Rawlsian maximin: improving the worst-off group’s performance).\n",
        "   * General evaluation platforms like Dynabench and the HELM framework provide dynamic, multi-metric leaderboards for robustness and human-in-the-loop assessment .\n"
      ],
      "metadata": {
        "id": "fs5sdnAdCJzI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 10.5 Dealing with Scale"
      ],
      "metadata": {
        "id": "bWSWJQjxDjJn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "Large language models (LLMs) pose unique challenges and opportunities due to their enormous size. Section 10.5 covers three core topics: **scaling laws**, the **KV cache**, and **parameter-efficient fine-tuning**.\n",
        "\n",
        "---\n",
        "\n",
        "### 10.5.1 Scaling Laws\n",
        "\n",
        "LLM performance (in terms of loss) follows empirical **power-law relationships** with three main factors:\n",
        "\n",
        "1. **Model size (N)** – number of non-embedding parameters\n",
        "2. **Dataset size (D)** – total number of training tokens\n",
        "3. **Compute budget (C)** – total FLOP-days spent training\n",
        "\n",
        "Kaplan et al. (2020) showed that, when holding two factors constant, loss $L$ scales as\n",
        "\n",
        "$$\n",
        "L(N) = \\Bigl(\\tfrac{N_c}{N}\\Bigr)^{\\alpha_N},\\quad\n",
        "L(D) = \\Bigl(\\tfrac{D_c}{D}\\Bigr)^{\\alpha_D},\\quad\n",
        "L(C) = \\Bigl(\\tfrac{C_c}{C}\\Bigr)^{\\alpha_C}.\n",
        "$$\n",
        "\n",
        "Here, $N_c,D_c,C_c$ and exponents $\\alpha_N,\\alpha_D,\\alpha_C$ depend on architecture and data specifics, but the power-law form guides decisions like whether adding more parameters or data yields better returns on loss reduction .\n",
        "\n",
        "A back-of-the-envelope formula for parameter count in a Transformer (ignoring biases) is\n",
        "\n",
        "$$\n",
        "N \\approx 12 \\times n_\\text{layer} \\times d^2,\n",
        "$$\n",
        "\n",
        "so GPT-3’s 175 B parameters arise from roughly 96 layers and $d\\approx12{,}288$ .\n",
        "\n",
        "---\n",
        "\n",
        "### 10.5.2 KV Cache\n",
        "\n",
        "During **inference**, autoregressive generation processes one token at a time. Recomputing every key and value for all prior tokens would be prohibitively expensive. Instead, models maintain a **key–value cache**:\n",
        "\n",
        "* When a token is processed, its key and value vectors are stored (“cached”).\n",
        "* For the next token, only the new query vector is computed; past keys/values are looked up rather than recomputed.\n",
        "\n",
        "This yields constant-time per-token overhead (excluding the cost of the new token) and enables efficient long-context generation .\n",
        "\n",
        "---\n",
        "\n",
        "### 10.5.3 Parameter-Efficient Fine-Tuning\n",
        "\n",
        "While full fine-tuning updates all model weights, **parameter-efficient methods** (e.g., adapters, LoRA) freeze the bulk of the pretrained parameters and train only small, added modules. This reduces:\n",
        "\n",
        "* **Compute**: far fewer parameters to update\n",
        "* **Storage**: only the small adapters need to be saved per task\n",
        "* **Data requirements**: often works well with limited task-specific data\n",
        "\n",
        "These approaches enable task specialization without the expense of full-model finetuning.\n",
        "\n",
        "---\n",
        "\n",
        "Together, scaling laws inform *whether* to grow models or data, KV caching makes *inference* tractable at scale, and parameter-efficient fine-tuning makes *adaptation* affordable.\n"
      ],
      "metadata": {
        "id": "unU0rkAjDdmI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 10.6 – Potential Harms from Language Models"
      ],
      "metadata": {
        "id": "Jz05qnntD0jb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "Large pretrained neural language models carry forward—and often amplify—a range of risks first noted for simpler NLP systems (e.g., in Chapters 4 and 6). These harms become especially acute whenever models generate text, whether in chatbots, summarizers, translation systems, or autocomplete. Key categories include:\n",
        "\n",
        "1. **Hallucinations (Factual Errors)**\n",
        "   LLMs optimize for *predictability* and *fluency*, not truth. As a result, they frequently produce plausible-sounding but incorrect “facts,” a phenomenon known as **hallucination**. For any application demanding factual reliability—legal advice, medical guidance, news summarization—hallucinations pose a severe danger .\n",
        "\n",
        "2. **Toxicity and Stereotyping**\n",
        "   Even benign prompts can elicit hate speech, slurs, or abusive language. Gehman et al. (2020) demonstrated that LLMs trained on web-scraped data can output toxic content or reinforce negative stereotypes about demographic groups, including subtle biases in sentiment or portrayal . Subsequent work (Cheng et al., 2023; Sheng et al., 2019) confirms these models perpetuate—and sometimes intensify—societal biases.\n",
        "\n",
        "3. **Data Biases and Amplification**\n",
        "\n",
        "   * **Source Bias:** Pretraining corpora disproportionately reflect text produced in developed countries, skewing generational perspectives toward those regions.\n",
        "   * **Amplification:** LLMs not only mirror but *exaggerate* biases present in their training data—much as embedding models do (see Chapter 6). For instance, toxic or stereotyped usage in a small subset of the data can become far more prevalent in generated text .\n",
        "\n",
        "4. **Privacy and Data Leakage**\n",
        "   Web-scraped datasets often include personally identifiable information—names, phone numbers, addresses—that models can inadvertently memorize. Adversaries have demonstrated successful extraction of such data from model parameters, a risk magnified when models are trained on sensitive records like electronic health information .\n",
        "\n",
        "5. **Misinformation & Malicious Use**\n",
        "   Beyond unintentional errors, LLMs serve as potent tools for generating convincing misinformation, phishing messages, extremist propaganda, and other malicious content. McGuffie & Newhouse (2020) show how LLMs can emulate extremist rhetoric, aiding radicalization efforts .\n",
        "\n",
        "6. **Copyright & Legal Concerns**\n",
        "   Training on massive web-scrapes raises thorny questions around copyright infringement and data consent. Although “fair use” provides some latitude, the legal status of large-scale web data remains unsettled, and sensitive or restricted content may slip through filtering processes (see Section 10.3.2).\n",
        "\n",
        "7. **Regulatory and Ethical Transparency**\n",
        "   As awareness of these harms grows, so do calls for **datasheets** and **model cards**—standardized documentation detailing training corpora, filtering methods, known biases, intended uses, and evaluation results. Such transparency aids both users and regulators in assessing model risks and compliance, and is increasingly mandated by emerging legislation .\n",
        "\n",
        "---\n",
        "\n",
        "*Taken together, these risks underscore the need for rigorous mitigation strategies—data curation and filtering, bias audits, retrieval-augmented architectures (to anchor generation in trusted sources), differential privacy techniques, and transparent documentation—before deploying LLMs in real-world settings.*\n"
      ],
      "metadata": {
        "id": "H81LlWMJDyX7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uuZO2Q-oc-b6"
      },
      "outputs": [],
      "source": []
    }
  ]
}