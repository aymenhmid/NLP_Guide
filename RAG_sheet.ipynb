{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNmg/FYj8AnsFhDo03v9WlV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aymenhmid/NLP_Guide/blob/main/RAG_sheet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## 1. Document Ingestion & Preprocessing\n",
        "\n",
        "1. **Chunking**\n",
        "\n",
        "   * **Why overlap?** Preserves context across splits.\n",
        "   * **Typical parameters**:  chunk size = 500 tokens, overlap = 100 tokens.\n",
        "   * **Python (using HuggingFace’s `tokenizers`)**:\n",
        "\n",
        "     ```python\n",
        "     from transformers import AutoTokenizer\n",
        "     tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "     def chunk_text(text, chunk_size=500, overlap=100):\n",
        "         tokens = tokenizer.encode(text)\n",
        "         chunks = []\n",
        "         for i in range(0, len(tokens), chunk_size - overlap):\n",
        "             chunk = tokens[i : i + chunk_size]\n",
        "             chunks.append(tokenizer.decode(chunk))\n",
        "         return chunks\n",
        "     ```\n",
        "\n",
        "2. **Embedding**\n",
        "\n",
        "   * **Models**: OpenAI’s `text-embedding-ada-002`, SF’s `all-mpnet-base-v2`, Cohere’s semantic embeddings.\n",
        "   * **Batching**: Send chunks in batches of 100–500 to avoid rate limits.\n",
        "\n",
        "3. **Indexing into a Vector Store**\n",
        "\n",
        "   * **Popular choices**:\n",
        "\n",
        "     * **FAISS** (on-prem, GPU-accelerated)\n",
        "     * **ChromaDB** (open-source, Python-native)\n",
        "     * **Pinecone / Weaviate** (managed)\n",
        "   * **Example (Chroma)**:\n",
        "\n",
        "     ```python\n",
        "     import chromadb\n",
        "     client = chromadb.Client()\n",
        "     collection = client.create_collection(\"my_docs\")\n",
        "\n",
        "     # after embedding each chunk to a vector `vec`\n",
        "     collection.add(\n",
        "         ids=[chunk_id],\n",
        "         metadatas=[{\"source\": filename, \"page\": page_number}],\n",
        "         embeddings=[vec],\n",
        "         documents=[chunk_text],\n",
        "     )\n",
        "     ```\n",
        "\n",
        "---\n",
        "\n",
        "## 2. Query Encoding & Retrieval\n",
        "\n",
        "1. **Query Encoder**\n",
        "\n",
        "   * Often reuses the same embedding model as the docs for vector space alignment.\n",
        "\n",
        "2. **Retrieval**\n",
        "\n",
        "   * **k-NN search**: retrieve top-k (e.g. k = 5–10).\n",
        "   * **Re-ranking**\n",
        "\n",
        "     * **Cross-encoder** (e.g. SBERT cross-encoder) to reorder your top-k by deeper semantic match.\n",
        "     * **Example**:\n",
        "\n",
        "       ```python\n",
        "       from sentence_transformers import CrossEncoder\n",
        "\n",
        "       cross_encoder = CrossEncoder(\"cross-encoder/ms-marco-MiniLM-L-6-v2\")\n",
        "       scores = cross_encoder.predict([\n",
        "           (query, doc_text) for doc_text in retrieved_texts\n",
        "       ])\n",
        "       ranked = [doc for _, doc in sorted(zip(scores, retrieved_texts), reverse=True)]\n",
        "       ```\n",
        "\n",
        "3. **Hybrid Retrieval**\n",
        "\n",
        "   * **Combine sparse + dense**: fuse BM25 (Elasticsearch) scores with vector similarity to boost exact-match signals.\n",
        "\n",
        "---\n",
        "\n",
        "## 3. LLM Prompting & Generation\n",
        "\n",
        "1. **Prompt Templates**\n",
        "\n",
        "   ```jinja\n",
        "   You are an expert assistant. Use the following context to answer:\n",
        "\n",
        "   Context:\n",
        "   {{retrieved_chunks}}\n",
        "\n",
        "   Question:\n",
        "   {{user_query}}\n",
        "\n",
        "   Answer:\n",
        "   ```\n",
        "\n",
        "2. **Temperature & Max Tokens**\n",
        "\n",
        "   * **Temperature** ≈ 0.0–0.3 for factual tasks.\n",
        "   * **Max tokens** tuned to ensure room for citations.\n",
        "\n",
        "3. **Streaming vs. Batched**\n",
        "\n",
        "   * **Streaming** for low latency in chat.\n",
        "   * **Batched** when you want the full answer before displaying.\n",
        "\n",
        "4. **Citation Injection**\n",
        "\n",
        "   * Append `[source: <metadata.source> | page <metadata.page>]` after each fact.\n",
        "   * Can be done via post-processing on the LLM’s output.\n",
        "\n",
        "---\n",
        "\n",
        "## 4. System Architecture & Scaling\n",
        "\n",
        "1. **Microservices**\n",
        "\n",
        "   * **“Retriever” service**: handles embedding & vector DB reads.\n",
        "   * **“Generator” service**: calls the LLM API.\n",
        "\n",
        "2. **Caching**\n",
        "\n",
        "   * Cache popular queries + retrieved contexts to avoid re-embedding.\n",
        "   * Use Redis with TTL ≈ 24 h for semi-static knowledge bases.\n",
        "\n",
        "3. **Monitoring**\n",
        "\n",
        "   * Track **latency** (split between retrieval vs. generation).\n",
        "   * Track **recall\\@k** on held-out Q\\&A pairs.\n",
        "\n",
        "4. **Security & Privacy**\n",
        "\n",
        "   * Encrypt proprietary docs at rest.\n",
        "   * Sanitize user inputs to avoid prompt injections.\n",
        "\n",
        "---\n",
        "\n",
        "## 5. Evaluation & Feedback Loop\n",
        "\n",
        "1. **Automatic Metrics**\n",
        "\n",
        "   * **Recall\\@k**: Did the golden answer’s chunk appear?\n",
        "   * **EM / F1** on generated answers vs. ground truth.\n",
        "\n",
        "2. **Human-in-the-Loop**\n",
        "\n",
        "   * Deploy “thumbs up/down” on answers.\n",
        "   * Periodically re-fine-tune or re-rank based on user feedback.\n",
        "\n",
        "3. **Continuous Index Refresh**\n",
        "\n",
        "   * For dynamic corpora (e.g. news), schedule daily or hourly re-indexing.\n",
        "\n",
        "---\n",
        "\n",
        "## 6. End-to-End LangChain Example\n",
        "\n",
        "```python\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "# 1. Embed + index (once)\n",
        "emb = OpenAIEmbeddings()\n",
        "faiss_index = FAISS.from_texts(chunks, embedding=emb)\n",
        "\n",
        "# 2. Build retriever + QA chain\n",
        "retriever = faiss_index.as_retriever(search_kwargs={\"k\": 7})\n",
        "llm = OpenAI(temperature=0.1, max_tokens=512)\n",
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm=llm, chain_type=\"map_reduce\", retriever=retriever\n",
        ")\n",
        "\n",
        "# 3. Ask a question\n",
        "answer = qa_chain.run(\"What are the main challenges of RAG systems?\")\n",
        "print(answer)\n",
        "```"
      ],
      "metadata": {
        "id": "3MXR9DbzGKk8"
      }
    }
  ]
}